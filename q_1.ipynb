{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alireza\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchtext\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path):\n",
    "\n",
    "  def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "  def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "      if len(review) != 0:\n",
    "        features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "  # Reading data from disk\n",
    "  df = pd.read_csv(csv_path)\n",
    "  # Replacing string labels with numbers\n",
    "  df['sentiment'].replace({'positive': 1, 'negative': 0}, inplace=True)\n",
    "\n",
    "  # dividing the dataset into train, validation, and test portions\n",
    "  X,y = df['review'].values,df['sentiment'].values\n",
    "  x_train1, x_test1, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=1)\n",
    "  x_train1, x_val1, y_train, y_val = train_test_split(x_train1, y_train, train_size=0.823, random_state=1)\n",
    "\n",
    "  print(f'Number of training examples: {len(x_train1)}')\n",
    "  print(f'Number of validation examples: {len(x_val1)}')\n",
    "  print(f'Number of test examples: {len(x_test1)}')\n",
    "\n",
    "\n",
    "  # Building the vocabulary\n",
    "  glove = GloVe(name='6B', dim=100, max_vectors=25000, cache = \"./vectors_cache\")\n",
    "\n",
    "  x_train, x_val, x_test = [],[],[]\n",
    "  for sent in x_train1:\n",
    "    x_train.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  for sent in x_val1:\n",
    "    x_val.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  for sent in x_test1:\n",
    "    x_test.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  # Padding the data\n",
    "  x_train_pad = padding_(x_train,500)\n",
    "  x_val_pad = padding_(x_val,500)\n",
    "  x_test_pad = padding_(x_test,500)\n",
    "\n",
    "  # creating Tensor Datasets and DataLoaders\n",
    "  train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "  valid_data = TensorDataset(torch.from_numpy(x_val_pad), torch.from_numpy(y_val))\n",
    "  test_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "  batch_size = 100\n",
    "\n",
    "  train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "  valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "  test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "  return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 34977\n",
      "Number of validation examples: 7523\n",
      "Number of test examples: 7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./vectors_cache\\glove.6B.zip: 862MB [02:50, 5.05MB/s]                               \n",
      "100%|█████████▉| 24999/25000 [00:01<00:00, 21624.36it/s]\n"
     ]
    }
   ],
   "source": [
    "trainloader, validloader, testloader = load_data(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
