{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the vocabulary\n",
    "glove = GloVe(name='6B', dim=100, max_vectors=25000, cache = \"./vectors_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path, glove):\n",
    "\n",
    "  def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "  def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "      if len(review) != 0:\n",
    "        features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "  # Reading data from disk\n",
    "  df = pd.read_csv(csv_path)\n",
    "  # Replacing string labels with numbers\n",
    "  df['sentiment'].replace({'positive': 1, 'negative': 0}, inplace=True)\n",
    "\n",
    "  # dividing the dataset into train, validation, and test portions\n",
    "  X,y = df['review'].values,df['sentiment'].values\n",
    "  x_train1, x_test1, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=1)\n",
    "  x_train1, x_val1, y_train, y_val = train_test_split(x_train1, y_train, train_size=0.823, random_state=1)\n",
    "\n",
    "  print(f'Number of training examples: {len(x_train1)}')\n",
    "  print(f'Number of validation examples: {len(x_val1)}')\n",
    "  print(f'Number of test examples: {len(x_test1)}')\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  x_train, x_val, x_test = [],[],[]\n",
    "  for sent in x_train1:\n",
    "    x_train.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  for sent in x_val1:\n",
    "    x_val.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  for sent in x_test1:\n",
    "    x_test.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  # Padding the data\n",
    "  x_train_pad = padding_(x_train,500)\n",
    "  x_val_pad = padding_(x_val,500)\n",
    "  x_test_pad = padding_(x_test,500)\n",
    "\n",
    "  # creating Tensor Datasets and DataLoaders\n",
    "  train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "  valid_data = TensorDataset(torch.from_numpy(x_val_pad), torch.from_numpy(y_val))\n",
    "  test_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "  batch_size = 100\n",
    "\n",
    "  train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "  valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "  test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "  return train_loader, valid_loader, test_loader\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()  \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion):\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  model.train()\n",
    "\n",
    "  for inputs, labels in tqdm(data_loader):\n",
    "      optimizer.zero_grad()\n",
    "      inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float)   \n",
    "      predictions = model(inputs).squeeze(1)\n",
    "      loss = criterion(predictions, labels)      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      acc = binary_accuracy(predictions, labels)\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "      \n",
    "  return epoch_loss / len(data_loader), epoch_acc / len(data_loader)\n",
    "\n",
    "def evaluate_epoch(model, data_loader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float)\n",
    "            predictions = model(inputs).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader)\n",
    "\n",
    "def count_recurrent_layer_parameters(model):\n",
    "    return sum(p.numel() for n, p in model.named_parameters() if p.requires_grad and 'fc' not in n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 34977\n",
      "Number of validation examples: 7523\n",
      "Number of test examples: 7500\n"
     ]
    }
   ],
   "source": [
    "trainloader, validloader, testloader = load_data(\"IMDB Dataset.csv\", glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBi2Level(nn.Module):\n",
    "  def __init__(self, hidden_dim, embed_dim):\n",
    "    super(RNNBi2Level, self).__init__()\n",
    "    \n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "\n",
    "    self.rnn_bi_2_level = nn.RNN(\n",
    "      input_size=self.embed_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      bidirectional=True,\n",
    "      num_layers=2,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    embeds = self.embedding(x)\n",
    "    output, hidden = self.rnn_bi_2_level(embeds)\n",
    "\n",
    "    # hidden shape = [n_directions * n_layers, batch_size, hidden_dim]\n",
    "    # Concatinate the last forward layer, and the last backward layer\n",
    "    hidden = torch.cat((hidden[2, :, :], hidden[3, :, :]), dim=1)\n",
    "\n",
    "    logit = self.fc(hidden)\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNBi2Level(embed_dim=100, hidden_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2 Level Bidirectional RNN model has 361,600 parameters in the two bi-RNN layers\n"
     ]
    }
   ],
   "source": [
    "print(f'The 2 Level Bidirectional RNN model has {count_recurrent_layer_parameters(model):,} parameters in the two bi-RNN layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 107/350 [02:30<05:41,  1.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alirezachoubineh/Desktop/Uni/COMP 5900 E Advanced Machine Learning/Assignments/2/AML-A2/q_1.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_epoch(model, trainloader, optimizer, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m evaluate(model, validloader, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32m/Users/alirezachoubineh/Desktop/Uni/COMP 5900 E Advanced Machine Learning/Assignments/2/AML-A2/q_1.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)   \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m predictions \u001b[39m=\u001b[39m model(inputs)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, labels)      \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/alirezachoubineh/Desktop/Uni/COMP 5900 E Advanced Machine Learning/Assignments/2/AML-A2/q_1.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn_bi_2_level(embeds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   \u001b[39m# hidden shape = [n_directions * n_layers, batch_size, hidden_dim]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m   \u001b[39m# Concatinate the last forward layer, and the last backward layer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alirezachoubineh/Desktop/Uni/COMP%205900%20E%20Advanced%20Machine%20Learning/Assignments/2/AML-A2/q_1.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m   hidden \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((hidden[\u001b[39m2\u001b[39m, :, :], hidden[\u001b[39m3\u001b[39m, :, :]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/rnn.py:553\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 553\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mrnn_tanh(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    554\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional,\n\u001b[1;32m    555\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    556\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mrnn_relu(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    558\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional,\n\u001b[1;32m    559\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "optimizer = Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, trainloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_epoch(model, validloader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate_epoch(model, testloader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
