{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alireza\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchtext\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the vocabulary\n",
    "glove = GloVe(name='6B', dim=100, max_vectors=25000, cache = \"./vectors_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path, glove):\n",
    "\n",
    "  def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "  def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "      if len(review) != 0:\n",
    "        features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "  # Reading data from disk\n",
    "  df = pd.read_csv(csv_path)\n",
    "  # Replacing string labels with numbers\n",
    "  df['sentiment'].replace({'positive': 1, 'negative': 0}, inplace=True)\n",
    "\n",
    "  # dividing the dataset into train, validation, and test portions\n",
    "  X,y = df['review'].values,df['sentiment'].values\n",
    "  x_train1, x_test1, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=1)\n",
    "  x_train1, x_val1, y_train, y_val = train_test_split(x_train1, y_train, train_size=0.823, random_state=1)\n",
    "\n",
    "  print(f'Number of training examples: {len(x_train1)}')\n",
    "  print(f'Number of validation examples: {len(x_val1)}')\n",
    "  print(f'Number of test examples: {len(x_test1)}')\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  x_train, x_val, x_test = [],[],[]\n",
    "  for sent in x_train1:\n",
    "    x_train.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  for sent in x_val1:\n",
    "    x_val.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  for sent in x_test1:\n",
    "    x_test.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  # Padding the data\n",
    "  x_train_pad = padding_(x_train,500)\n",
    "  x_val_pad = padding_(x_val,500)\n",
    "  x_test_pad = padding_(x_test,500)\n",
    "\n",
    "  # creating Tensor Datasets and DataLoaders\n",
    "  train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "  valid_data = TensorDataset(torch.from_numpy(x_val_pad), torch.from_numpy(y_val))\n",
    "  test_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "  batch_size = 100\n",
    "\n",
    "  train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "  valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "  test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "  return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 34977\n",
      "Number of validation examples: 7523\n",
      "Number of test examples: 7500\n"
     ]
    }
   ],
   "source": [
    "trainloader, validloader, testloader = load_data(\"IMDB Dataset.csv\", glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBi2Level(nn.Module):\n",
    "  def __init__(self, hidden_dim, embed_dim):\n",
    "    super(RNNBi2Level, self).__init__()\n",
    "    \n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "\n",
    "    self.rnn_bi_2_level = nn.RNN(\n",
    "      input_size=self.embed_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      bidirectional=True,\n",
    "      num_layers=2\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBi(nn.Module):\n",
    "  def __init__(self, hidden_dim, embed_dim):\n",
    "    super(RNNBi, self).__init__()\n",
    "    \n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "\n",
    "    self.rnn_bi = nn.RNN(\n",
    "      input_size=self.embed_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      bidirectional=True,\n",
    "      num_layers=1\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBi(nn.Module):\n",
    "  def __init__(self, hidden_dim, embed_dim):\n",
    "    super(LSTMBi, self).__init__()\n",
    "    \n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "\n",
    "    self.lstm_bi = nn.LSTM(\n",
    "      input_size=self.embed_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      bidirectional=True,\n",
    "      num_layers=1\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  def __init__(self, hidden_dim, embed_dim):\n",
    "    super(LSTM, self).__init__()\n",
    "    \n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "\n",
    "    self.rnn_bi_2_level = nn.RNN(\n",
    "      input_size=self.embed_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      bidirectional=False,\n",
    "      num_layers=1\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBi2Level(nn.Module):\n",
    "  def __init__(self, hidden_dim, embed_dim):\n",
    "    super(LSTMBi2Level, self).__init__()\n",
    "    \n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "\n",
    "    self.rnn_bi_2_level = nn.LSTM(\n",
    "      input_size=self.embed_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      bidirectional=True,\n",
    "      num_layers=2\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_bi_rnn_2_layers = RNNBi2Level(embed_dim=100, hidden_dim=200)\n",
    "net_bi_rnn = RNNBi(embed_dim=100, hidden_dim=200)\n",
    "net_bi_lstm_2_layers = LSTMBi2Level(embed_dim=100, hidden_dim=200)\n",
    "net_bi_lstm = LSTMBi(embed_dim=100, hidden_dim=200)\n",
    "net_lstm = LSTM(embed_dim=100, hidden_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model BI RNN 2 Layers has 362,001 trainable parameters\n",
      "The model BI RNN 1 Layer has 121,201 trainable parameters\n",
      "The model BI LSTM 2 Layers has 1,446,801 trainable parameters\n",
      "The model BI LSTM 1 Layer has 483,601 trainable parameters\n",
      "The model LSTM 1 Layer has 60,801 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model BI RNN 2 Layers has {count_parameters(net_bi_rnn_2_layers):,} trainable parameters')\n",
    "print(f'The model BI RNN 1 Layer has {count_parameters(net_bi_rnn):,} trainable parameters')\n",
    "print(f'The model BI LSTM 2 Layers has {count_parameters(net_bi_lstm_2_layers):,} trainable parameters')\n",
    "print(f'The model BI LSTM 1 Layer has {count_parameters(net_bi_lstm):,} trainable parameters')\n",
    "print(f'The model LSTM 1 Layer has {count_parameters(net_lstm):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <td>Model</td>\n",
    "    <td># Parameters</td>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>2-level-bi-LSTM</td>\n",
    "    <td>1,446,400</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2-level-bi-RNN</td>\n",
    "    <td>361,600</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>bi-RNN</td>\n",
    "    <td>120,800</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>bi-LSTM</td>\n",
    "    <td>483,200</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>483,200</td>\n",
    "    <td>241,600</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>RNN</td>\n",
    "    <td>60,400</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
