{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the vocabulary\n",
    "glove = GloVe(name='6B', dim=100, max_vectors=25000, cache = \"./vectors_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path, glove):\n",
    "\n",
    "  def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "  def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "      if len(review) != 0:\n",
    "        features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "  # Reading data from disk\n",
    "  df = pd.read_csv(csv_path)\n",
    "  # Replacing string labels with numbers\n",
    "  df['sentiment'].replace({'positive': 1, 'negative': 0}, inplace=True)\n",
    "\n",
    "  # dividing the dataset into train, validation, and test portions\n",
    "  X,y = df['review'].values,df['sentiment'].values\n",
    "  x_train1, x_test1, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=1)\n",
    "  x_train1, x_val1, y_train, y_val = train_test_split(x_train1, y_train, train_size=0.823, random_state=1)\n",
    "\n",
    "  print(f'Number of training examples: {len(x_train1)}')\n",
    "  print(f'Number of validation examples: {len(x_val1)}')\n",
    "  print(f'Number of test examples: {len(x_test1)}')\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  x_train, x_val, x_test = [],[],[]\n",
    "  for sent in x_train1:\n",
    "    x_train.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  for sent in x_val1:\n",
    "    x_val.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  for sent in x_test1:\n",
    "    x_test.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  # Padding the data\n",
    "  x_train_pad = padding_(x_train,500)\n",
    "  x_val_pad = padding_(x_val,500)\n",
    "  x_test_pad = padding_(x_test,500)\n",
    "\n",
    "  # creating Tensor Datasets and DataLoaders\n",
    "  train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "  valid_data = TensorDataset(torch.from_numpy(x_val_pad), torch.from_numpy(y_val))\n",
    "  test_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "  batch_size = 100\n",
    "\n",
    "  train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "  valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "  test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "  return train_loader, valid_loader, test_loader\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()  \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion):\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  model.train()\n",
    "\n",
    "  for inputs, labels in tqdm(data_loader):\n",
    "      optimizer.zero_grad()\n",
    "      inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float)   \n",
    "      predictions = model(inputs).squeeze(1)\n",
    "      loss = criterion(predictions, labels)      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      acc = binary_accuracy(predictions, labels)\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "      \n",
    "  return epoch_loss / len(data_loader), epoch_acc / len(data_loader)\n",
    "\n",
    "def evaluate_epoch(model, data_loader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float)\n",
    "            predictions = model(inputs).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader)\n",
    "\n",
    "def count_recurrent_layer_parameters(model):\n",
    "    return sum(p.numel() for n, p in model.named_parameters() if p.requires_grad and 'fc' not in n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, validloader, testloader = load_data(\"IMDB Dataset.csv\", glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBi(nn.Module):\n",
    "  def __init__(self, hidden_dim, embed_dim):\n",
    "    super(LSTMBi, self).__init__()\n",
    "    \n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "\n",
    "    self.rnn_bi_2_level = nn.LSTM(\n",
    "      input_size=self.embed_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      bidirectional=True,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    embeds = self.embedding(x)\n",
    "    output, hidden = self.rnn_bi_2_level(embeds)\n",
    "\n",
    "    # hidden shape = [n_directions * n_layers, batch_size, hidden_dim]\n",
    "    # Concatinate the last forward layer, and the last backward layer\n",
    "    hidden = torch.cat((hidden[0, :, :], hidden[1, :, :]), dim=1)\n",
    "\n",
    "    logit = self.fc(hidden)\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMBi(embed_dim=100, hidden_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The 2 Level Bidirectional RNN model has {count_recurrent_layer_parameters(model):,} parameters in the two bi-RNN layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model and evaluating the validation data per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "optimizer = Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, trainloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_epoch(model, validloader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate_epoch(model, testloader, criterion)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
