{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alireza\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the vocabulary\n",
    "glove = GloVe(name='6B', dim=100, max_vectors=25000, cache = \"./vectors_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path, glove):\n",
    "\n",
    "  def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "  def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "      if len(review) != 0:\n",
    "        features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "  # Reading data from disk\n",
    "  df = pd.read_csv(csv_path)\n",
    "  # Replacing string labels with numbers\n",
    "  df['sentiment'].replace({'positive': 1, 'negative': 0}, inplace=True)\n",
    "\n",
    "  # dividing the dataset into train, validation, and test portions\n",
    "  X,y = df['review'].values,df['sentiment'].values\n",
    "  x_train1, x_test1, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=1)\n",
    "  x_train1, x_val1, y_train, y_val = train_test_split(x_train1, y_train, train_size=0.823, random_state=1)\n",
    "\n",
    "  print(f'Number of training examples: {len(x_train1)}')\n",
    "  print(f'Number of validation examples: {len(x_val1)}')\n",
    "  print(f'Number of test examples: {len(x_test1)}')\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  x_train, x_val, x_test = [],[],[]\n",
    "  for sent in x_train1:\n",
    "    x_train.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  for sent in x_val1:\n",
    "    x_val.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  for sent in x_test1:\n",
    "    x_test.append([glove.stoi[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in glove.stoi])\n",
    "\n",
    "  # Padding the data\n",
    "  x_train_pad = padding_(x_train,500)\n",
    "  x_val_pad = padding_(x_val,500)\n",
    "  x_test_pad = padding_(x_test,500)\n",
    "\n",
    "  # creating Tensor Datasets and DataLoaders\n",
    "  train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "  valid_data = TensorDataset(torch.from_numpy(x_val_pad), torch.from_numpy(y_val))\n",
    "  test_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "  batch_size = 100\n",
    "\n",
    "  train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "  valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "  test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "  return train_loader, valid_loader, test_loader\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()  \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion):\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  model.train()\n",
    "\n",
    "  for inputs, labels in tqdm(data_loader):\n",
    "      optimizer.zero_grad()\n",
    "      inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float)   \n",
    "      predictions = model(inputs).squeeze(1)\n",
    "      loss = criterion(predictions, labels)      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      acc = binary_accuracy(predictions, labels)\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "      \n",
    "  return epoch_loss / len(data_loader), epoch_acc / len(data_loader)\n",
    "\n",
    "def evaluate_epoch(model, data_loader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device, dtype=torch.float)\n",
    "            predictions = model(inputs).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader)\n",
    "\n",
    "def count_recurrent_layer_parameters(model):\n",
    "    return sum(p.numel() for n, p in model.named_parameters() if p.requires_grad and 'fc' not in n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 34977\n",
      "Number of validation examples: 7523\n",
      "Number of test examples: 7500\n"
     ]
    }
   ],
   "source": [
    "trainloader, validloader, testloader = load_data(\"IMDB Dataset.csv\", glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  def __init__(self, hidden_dim, embed_dim):\n",
    "    super(LSTM, self).__init__()\n",
    "    \n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "\n",
    "    self.lstm = nn.LSTM(\n",
    "      input_size=self.embed_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      bidirectional=False,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Linear(self.hidden_dim , 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    embeds = self.embedding(x)\n",
    "    output, (hidden, cell) = self.lstm(embeds)\n",
    "\n",
    "    logit = self.fc(hidden[0, :, :])\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(25000, 100)\n",
       "  (lstm): LSTM(100, 200, batch_first=True)\n",
       "  (fc): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM(embed_dim=100, hidden_dim=200)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2 Level Bidirectional RNN model has 241,600 parameters in the two bi-RNN layers\n"
     ]
    }
   ],
   "source": [
    "print(f'The 2 Level Bidirectional RNN model has {count_recurrent_layer_parameters(model):,} parameters in the two bi-RNN layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model and evaluating the validation data per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:19<00:00, 17.90it/s]\n",
      "100%|██████████| 76/76 [00:01<00:00, 46.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.62714 | Train Acc: 65.21%\n",
      "\t Val. Loss: 0.54435 |  Val. Acc: 72.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:20<00:00, 17.20it/s]\n",
      "100%|██████████| 76/76 [00:01<00:00, 45.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.51800 | Train Acc: 75.26%\n",
      "\t Val. Loss: 0.50160 |  Val. Acc: 76.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:19<00:00, 17.52it/s]\n",
      "100%|██████████| 76/76 [00:01<00:00, 46.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.50742 | Train Acc: 75.67%\n",
      "\t Val. Loss: 0.37680 |  Val. Acc: 83.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:19<00:00, 17.61it/s]\n",
      "100%|██████████| 76/76 [00:01<00:00, 44.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.36671 | Train Acc: 83.86%\n",
      "\t Val. Loss: 0.35908 |  Val. Acc: 85.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:20<00:00, 17.50it/s]\n",
      "100%|██████████| 76/76 [00:01<00:00, 45.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.33073 | Train Acc: 85.97%\n",
      "\t Val. Loss: 0.32774 |  Val. Acc: 85.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "optimizer = Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, trainloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_epoch(model, validloader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        os.makedirs('lstm', exist_ok=True)\n",
    "        torch.save(model.state_dict(), 'lstm/best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:01<00:00, 45.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the LSTM is 0.8557333127657573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_epoch(model, testloader, criterion)\n",
    "print(f'Test accuracy of the LSTM is {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
